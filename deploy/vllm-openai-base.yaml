apiVersion: apps/v1
kind: StatefulSet
metadata:
  annotations:
    meta.helm.sh/release-name: vllm-openai-base
    meta.helm.sh/release-namespace: ai-algo
  labels:
    app: vllm-openai-base
    appType: vllm-openai-base
    group: ai-algo
  name: vllm-openai-base
  namespace: ai-algo
spec:
  serviceName: vllm-openai-base
  replicas: 1
  revisionHistoryLimit: 10
  selector:
    matchLabels:
      app: vllm-openai-base
  template:
    metadata:
      labels:
        app: vllm-openai-base
      name: vllm-openai-base
      namespace: ai-algo
    spec:
      containers:
        - command:
            - python3
            - -m
            - vllm.entrypoints.openai.api_server
          args:
            - --host
            - "0.0.0.0"
            - --port
            - "8080"
            - --model
            - "/home/model-server/models/checkpoint-966"
            - --tensor-parallel-size
            - "4"
            - --served-model-name
            - "water"
            - --uvicorn-log-level
            - "debug"
          env:
            - name: CUDA_VISIBLE_DEVICES
              value: "1,3,4,7"
            - name: VLLM_USE_MODELSCOPE
              value: "True"
          image: docker.shuiditech.com/devops/vllm-openai:shuidi-v0.5.0
          name: vllm-openai
          imagePullPolicy: Always
          ports:
            - containerPort: 8080
              name: http
              protocol: TCP
          resources:
            limits:
              cpu: "16"
#              nvidia.com/gpu: "4"
            requests:
              cpu: "1"
#              nvidia.com/gpu: "4"
          terminationMessagePath: /dev/termination-log
          terminationMessagePolicy: File
          volumeMounts:
            - name: workspace
              mountPath: /home/model-server/models
              subPath: ai-water-guard-model-base/baseline/models
            - mountPath: /dev/shm
              name: cache-volume
      dnsPolicy: ClusterFirst
      enableServiceLinks: true
      restartPolicy: Always
      affinity:
        nodeAffinity:
          requiredDuringSchedulingIgnoredDuringExecution:
            nodeSelectorTerms:
              - matchExpressions:
                  - key: kubernetes.io/hostname
                    operator: In
                    values:
                      - vm-50-16-tencentos
#                      - vm-50-7-tencentos
      volumes:
      - name: workspace
        persistentVolumeClaim:
          claimName: torchserve-ai-algo-cfs-pvc
      - emptyDir:
          medium: Memory
          sizeLimit: 16Gi
        name: cache-volume

---
apiVersion: v1
kind: Service
metadata:
  labels:
    app: vllm-openai-base
  name: vllm-openai-base
  namespace: ai-algo
spec:
  ports:
  - port: 8080
    name: http
    protocol: TCP
  selector:
    app: vllm-openai-base
  type: ClusterIP






